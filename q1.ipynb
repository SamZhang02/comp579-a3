{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import lib  #helper functions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T23:24:09.015702Z",
     "start_time": "2024-03-14T23:24:08.666839Z"
    }
   },
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DecayingEpsilon:\n",
    "  def __init__(self, initial_value=1, decay_per_step=0.99) -> None:\n",
    "    self.initial_value = initial_value\n",
    "    self.value = initial_value\n",
    "    self.decay_per_step = decay_per_step\n",
    "\n",
    "  def decay(self):\n",
    "    self.value *= self.decay_per_step"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T23:24:09.020115Z",
     "start_time": "2024-03-14T23:24:09.016840Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Agent(ABC):\n",
    "  def __init__(self, env: gym.Env, epsilon=0.1, alpha=0.1, gamma=1, num_tilings=8,\n",
    "               tiles_per_tiling=10):\n",
    "    self.env = env\n",
    "    self.epsilon = epsilon  # epsilon greedy\n",
    "    self.alpha = alpha/num_tilings  # learning rate\n",
    "    self.gamma = gamma  # discount factor\n",
    "    self.num_tilings = num_tilings\n",
    "    self.tiles_per_tiling = tiles_per_tiling\n",
    "\n",
    "    self.tile_coder = lib.TileCoder(self.num_tilings,\n",
    "                                    self.tiles_per_tiling,\n",
    "                                    list(zip(env.observation_space.low,\n",
    "                                             env.observation_space.high)))\n",
    "\n",
    "    self.thetas = None\n",
    "    self.reset()\n",
    "\n",
    "  def Q(self, state, action) -> float:\n",
    "    features = self.tile_coder.encode(state)  # binary vec\n",
    "\n",
    "    return np.sum(self.thetas[action] * features)\n",
    "\n",
    "  def select_action(self, state, greedy=False) -> int:\n",
    "    # epsilon greedy\n",
    "    epsilon = self.epsilon if not isinstance(self.epsilon,\n",
    "                                             DecayingEpsilon) else self.epsilon.value\n",
    "    to_explore = np.random.rand() < epsilon\n",
    "\n",
    "    if greedy or not to_explore:\n",
    "      return np.argmax([self.Q(state, a) for a in range(self.env.action_space.n)])\n",
    "    else:\n",
    "      return self.env.action_space.sample()\n",
    "\n",
    "  @abstractmethod\n",
    "  def update(self, state, action, reward, next_state, next_action, done) -> None:\n",
    "    pass\n",
    "\n",
    "  def reset(self):\n",
    "    # params setup\n",
    "    self.thetas = np.random.uniform(-0.001, 0.001,\n",
    "                                    (env.action_space.n, self.tile_coder.total_tiles))\n",
    "\n",
    "\n",
    "class ExpectedSARSA(Agent):\n",
    "\n",
    "  def update(self, state, action, reward, next_state, next_action, done) -> None:\n",
    "    y = np.mean(\n",
    "      [self.Q(next_state, a) for a in range(self.env.action_space.n)]) if not done else 0\n",
    "    y_hat = self.Q(state, action)\n",
    "\n",
    "    td_error = reward + self.gamma * y - y_hat if not done else reward\n",
    "    features = self.tile_coder.encode(state)\n",
    "\n",
    "    self.thetas[action] += self.alpha * td_error * features\n",
    "\n",
    "\n",
    "class QLearning(Agent):\n",
    "\n",
    "  def update(self, state, action, reward, next_state, next_action, done) -> None:\n",
    "    y_hat = self.Q(state, action)\n",
    "    y = max([self.Q(next_state, a) for a in range(self.env.action_space.n)]) if not done else 0\n",
    "\n",
    "    td_error = reward + self.gamma * y - y_hat if not done else reward\n",
    "    features = self.tile_coder.encode(state)\n",
    "\n",
    "    self.thetas[action] += self.alpha * td_error * features\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T23:24:09.024977Z",
     "start_time": "2024-03-14T23:24:09.022995Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def run_experiment(agent, num_runs, num_episodes):\n",
    "  performances_across_runs = np.zeros(num_episodes)\n",
    "\n",
    "  with tqdm(total=num_runs * num_episodes) as pbar:\n",
    "    for run in range(num_runs):\n",
    "      agent.reset()\n",
    "      for episode in range(num_episodes):\n",
    "        # initialize env \n",
    "        reward_throughout_ep = 0\n",
    "        state, info = agent.env.reset()\n",
    "        state = np.double(state)\n",
    "        is_terminal = False\n",
    "        truncated = False\n",
    "\n",
    "        while not is_terminal and not truncated:\n",
    "          action = agent.select_action(state)\n",
    "\n",
    "          next_state, reward, is_terminal, truncated, info = agent.env.step(action)\n",
    "          next_state = np.double(next_state)\n",
    "\n",
    "          reward_throughout_ep += reward\n",
    "\n",
    "          next_action = agent.select_action(next_state)\n",
    "\n",
    "          agent.update(state,\n",
    "                       action,\n",
    "                       reward,\n",
    "                       next_state,\n",
    "                       next_action,\n",
    "                       is_terminal or truncated)\n",
    "\n",
    "          state, action = next_state, next_action\n",
    "          \n",
    "          if isinstance(agent.epsilon, DecayingEpsilon):\n",
    "            agent.epsilon.decay()\n",
    "\n",
    "        performances_across_runs[episode] += reward_throughout_ep\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "  return performances_across_runs / num_runs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T23:24:09.030208Z",
     "start_time": "2024-03-14T23:24:09.026593Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samzhang/Documents/School/McGill/Winter 2024 /Comp 579/a3/lib.py:16: RuntimeWarning: overflow encountered in subtract\n",
      "  self.norm_dims = np.array(tiles_per_dim) / (self.limits[:, 1] - self.limits[:, 0])\n",
      "100%|██████████| 50000/50000 [00:46<00:00, 1064.41it/s]\n",
      "100%|██████████| 50000/50000 [01:39<00:00, 504.00it/s]\n",
      "100%|██████████| 50000/50000 [06:03<00:00, 137.68it/s]\n",
      "100%|██████████| 50000/50000 [02:35<00:00, 320.84it/s]\n",
      "100%|██████████| 50000/50000 [05:24<00:00, 154.17it/s]\n",
      "100%|██████████| 50000/50000 [05:45<00:00, 144.87it/s]\n",
      "100%|██████████| 50000/50000 [02:37<00:00, 317.99it/s]\n",
      "100%|██████████| 50000/50000 [04:59<00:00, 166.71it/s]\n",
      "100%|██████████| 50000/50000 [04:34<00:00, 182.17it/s]\n",
      "100%|██████████| 50000/50000 [09:00<00:00, 92.54it/s] \n",
      "100%|██████████| 50000/50000 [08:54<00:00, 93.47it/s] \n",
      " 34%|███▍      | 17184/50000 [03:03<06:04, 89.93it/s] "
     ]
    }
   ],
   "source": [
    "envs = [gym.make('CartPole-v1'), gym.make('MountainCar-v0')]\n",
    "agents = [ExpectedSARSA, QLearning]\n",
    "\n",
    "alphas = [1 / 4, 1 / 8, 1 / 16]\n",
    "\n",
    "epsilons = {\n",
    "  'CartPole-v1': [DecayingEpsilon(decay_per_step=0.995), 1 / 4, 1 / 8],\n",
    "  'MountainCar-v0': [DecayingEpsilon(decay_per_step=0.995), 1 / 8, 1 / 16]\n",
    "}\n",
    "\n",
    "gammas = {\n",
    "  'CartPole-v1': 0.99,\n",
    "  'MountainCar-v0': 1\n",
    "}\n",
    "\n",
    "tilings = {\n",
    "  'CartPole-v1': {\n",
    "    'tilings': 4,\n",
    "    'tiles': 22\n",
    "  },\n",
    "  'MountainCar-v0': {\n",
    "    'tilings': 7,\n",
    "    'tiles': 14\n",
    "  }\n",
    "}\n",
    "\n",
    "NUM_RUNS = 50\n",
    "NUM_EPISODES = 1000\n",
    "\n",
    "n_envs = len(envs)\n",
    "n_agents = len(agents)\n",
    "n_epsilons = len(epsilons)\n",
    "n_alphas = len(alphas)\n",
    "\n",
    "n_plots = n_envs * n_agents * n_epsilons * n_alphas\n",
    "\n",
    "fig, axs = plt.subplots(nrows=len(envs), ncols=len(agents), figsize=(15, 10))\n",
    "\n",
    "if len(envs) * len(agents) > 1:\n",
    "  axs = axs.flatten()\n",
    "\n",
    "for agent_idx, agent in enumerate(agents):\n",
    "  for env_idx, env in enumerate(envs):\n",
    "\n",
    "    env_name = env.unwrapped.spec.id\n",
    "    ax = axs[env_idx * len(agents) + agent_idx] if len(envs) * len(agents) > 1 else axs\n",
    "\n",
    "    for alpha in alphas:\n",
    "      for epsilon in epsilons[env_name]:\n",
    "        exp_agent = agent(env, \n",
    "                          epsilon=epsilon, \n",
    "                          alpha=alpha, \n",
    "                          gamma=gammas[env_name],\n",
    "                          num_tilings=tilings[env_name]['tilings'],\n",
    "                          tiles_per_tiling=tilings[env_name]['tiles'])\n",
    "        \n",
    "        results = run_experiment(exp_agent, NUM_RUNS, NUM_EPISODES)\n",
    "\n",
    "        if isinstance(epsilon, DecayingEpsilon):\n",
    "          label = f'Decaying ε= ${epsilon.initial_value} * {epsilon.decay_per_step}^x$, α={alpha}'\n",
    "        else:\n",
    "          label = f'ε={epsilon}, α={alpha/exp_agent.num_tilings}'\n",
    "\n",
    "        ax.plot(results, label=label)\n",
    "\n",
    "        std_results = np.std(results, axis=0)\n",
    "        ax.fill_between(range(NUM_EPISODES), results - std_results, results + std_results,\n",
    "                        alpha=0.1)\n",
    "\n",
    "    ax.set_title(\n",
    "      f'{agent.__name__} - {env.spec.id} - {tilings[env_name][\"tilings\"]} tilings - {tilings[env_name][\"tiles\"]} tiles')\n",
    "    ax.set_xlabel('Episodes')\n",
    "    ax.set_ylabel('Average Return')\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "# Add an overall title and adjust layout\n",
    "plt.suptitle('Performance of Expected SARSA and Q-Learning with Different ε and α')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-14T23:24:09.031771Z"
    }
   },
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
