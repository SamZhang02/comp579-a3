{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e263c97c3bcfe02c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T20:42:46.729263Z",
     "start_time": "2024-03-14T20:42:46.712178Z"
    }
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import lib  #helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Temperature:\n",
    "  \"\"\"\n",
    "  Wrapper class for a temperature value, with optional decaying\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, value, decay=1):\n",
    "    self.initial_value = value\n",
    "    self.value = value\n",
    "    self.decay = decay\n",
    "\n",
    "  def decay(self):\n",
    "    self.value = self.value * self.decay\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T21:14:59.946073Z",
     "start_time": "2024-03-14T21:14:59.940495Z"
    }
   },
   "id": "88df16611b95f3ed",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Agent(ABC):\n",
    "  def __init__(self, env: gym.Env, temp=Temperature(1), alpha=0.1, gamma=1, num_tilings=8,\n",
    "               tiles_per_tiling=10):\n",
    "    self.env = env\n",
    "    self.temp = temp\n",
    "    self.alpha = alpha  # learning rate\n",
    "    self.gamma = gamma  # discount factor\n",
    "    self.num_tilings = num_tilings\n",
    "    self.tiles_per_tiling = tiles_per_tiling\n",
    "\n",
    "    self.tile_coder = lib.TileCoder(self.num_tilings,\n",
    "                                    self.tiles_per_tiling,\n",
    "                                    list(zip(env.observation_space.low,\n",
    "                                             env.observation_space.high)))\n",
    "\n",
    "    self.thetas = None\n",
    "    self.reset()\n",
    "\n",
    "  def z(self, state, action) -> float:\n",
    "    features = self.tile_coder.encode(state)  # binary vec\n",
    "\n",
    "    return np.sum(self.thetas[action] * features)\n",
    "\n",
    "  def select_action(self, state, temp=1) -> int:\n",
    "    z_values = np.array([self.z(state, a) for a in range(self.env.action_space.n)])\n",
    "    probabilities = np.exp(z_values / temp) / np.sum(np.exp(z_values / temp))\n",
    "\n",
    "    action = np.random.choice(self.env.action_space.n, p=probabilities)\n",
    "    return action\n",
    "\n",
    "  @abstractmethod\n",
    "  def update(self, state, action, reward, next_state, next_action, done) -> None:\n",
    "    pass\n",
    "\n",
    "  def reset(self):\n",
    "    # params setup\n",
    "    self.thetas = np.random.uniform(-0.001, 0.001,\n",
    "                                    (env.action_space.n, self.tile_coder.total_tiles))\n",
    "\n",
    "\n",
    "Trajectory = List[Tuple['state_t', 'action_t', 'reward_t']]\n",
    "\n",
    "\n",
    "class REINFORCE(Agent):\n",
    "  def __init__(self, env: gym.Env, temp=Temperature(1), alpha=0.1, gamma=1, num_tilings=8,\n",
    "               tiles_per_tiling=10):\n",
    "\n",
    "    super().__init__(env,\n",
    "                     temp=temp,\n",
    "                     alpha=alpha,\n",
    "                     gamma=gamma,\n",
    "                     num_tilings=num_tilings,\n",
    "                     tiles_per_tiling=tiles_per_tiling)\n",
    "\n",
    "    self.trajectory: Trajectory = list()\n",
    "\n",
    "    def update(self, state, action, reward, next_state, next_action, done) -> None:\n",
    "      if not done:\n",
    "        agent.trajectory.append((state, action, reward))\n",
    "        return\n",
    "\n",
    "    G = 0  # Store cumulative return\n",
    "    for st, at, rt in reversed(self.trajectory):\n",
    "      G = self.gamma * G + rt  # Update return\n",
    "      features = self.tile_coder.encode(st)\n",
    "      # Compute gradient ascent update for theta\n",
    "      grad_ln_pi = features - np.sum(\n",
    "        [self.thetas[a] * features for a in range(self.env.action_space.n)], axis=0)\n",
    "      self.thetas[at] += self.alpha * grad_ln_pi * G\n",
    "\n",
    "    self.trajectory = []  # Clear trajectory\n",
    "\n",
    "\n",
    "class ActorCritic(Agent):\n",
    "  def update(self, state, action, reward, next_state, next_action, done) -> None:\n",
    "    # Get current estimate and next state estimate\n",
    "    current_value = self.z(state, action)\n",
    "    next_value = self.z(next_state, next_action) if not done else 0\n",
    "\n",
    "    td_err = reward + self.gamma * next_value - current_value\n",
    "\n",
    "    # Update critic\n",
    "    features = self.tile_coder.encode(state)\n",
    "    self.thetas[action] += self.alpha * td_err * features\n",
    "\n",
    "    # Update actor\n",
    "    # with softmax policy, derive the gradient with respect to theta\n",
    "    policy_val = np.sum([np.exp(self.z(state, a)) * self.thetas[a] * features for a in range(self.env.action_space.n)], axis=0) / np.sum([np.exp(self.z(state, a)) for a in range(self.env.action_space.n)])\n",
    "    grad_ln_pi = features - policy_val\n",
    "    self.thetas[action] += self.alpha * grad_ln_pi * td_err\n"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-14T21:15:05.206566Z",
     "start_time": "2024-03-14T21:15:05.200234Z"
    }
   },
   "id": "initial_id",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def run_experiment(agent, num_runs: int, num_episodes: int):\n",
    "  performances_across_runs = np.zeros(num_episodes)\n",
    "\n",
    "  with tqdm(total=num_runs * num_episodes) as pbar:\n",
    "    for run in range(num_runs):\n",
    "      agent.reset()\n",
    "      for episode in range(num_episodes):\n",
    "        # initialize env \n",
    "        reward_throughout_ep = 0\n",
    "        state, info = agent.env.reset()\n",
    "        state = np.double(state)\n",
    "        is_terminal = False\n",
    "        truncated = False\n",
    "\n",
    "        while not is_terminal and not truncated:\n",
    "          action = agent.select_action(state)\n",
    "\n",
    "          next_state, reward, is_terminal, truncated, info = agent.env.step(action)\n",
    "          next_state = np.double(next_state)\n",
    "\n",
    "          reward_throughout_ep += reward\n",
    "\n",
    "          next_action = agent.select_action(next_state)\n",
    "\n",
    "          agent.update(state,\n",
    "                       action,\n",
    "                       reward,\n",
    "                       next_state,\n",
    "                       next_action,\n",
    "                       is_terminal or truncated)\n",
    "\n",
    "          state, action = next_state, next_action\n",
    "\n",
    "        agent.temp.decay()\n",
    "        performances_across_runs[episode] += reward_throughout_ep\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "  return performances_across_runs / num_runs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T20:34:41.660813Z",
     "start_time": "2024-03-14T20:34:41.657401Z"
    }
   },
   "id": "efa5eeb27f026dbb",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "envs = [gym.make('CartPole-v1'), gym.make('MountainCar-v0')]\n",
    "agents = [REINFORCE, ActorCritic]\n",
    "temps = [Temperature(1), Temperature(10, decay=0.99)]\n",
    "\n",
    "NUM_RUNS = 5\n",
    "NUM_EPISODES = 10000\n",
    "\n",
    "alphas = {\n",
    "  'CartPole-v1': {\n",
    "    'REINFORCE': 0.001\n",
    "    'ActorCritic': 0.001\n",
    "  }\n",
    "  'MountainCar-v0': {\n",
    "    'REINFORCE': 0.001\n",
    "    'ActorCritic': 0.001\n",
    "  }\n",
    "}\n",
    "\n",
    "gammas = {\n",
    "  'CartPole-v1': 0.99,\n",
    "  'MountainCar-v0': 1\n",
    "}\n",
    "\n",
    "tilings = {\n",
    "  'CartPole-v1': {\n",
    "    'tilings': 4,\n",
    "    'tiles': 22\n",
    "  },\n",
    "  'MountainCar-v0': {\n",
    "    'tilings': 7,\n",
    "    'tiles': 14\n",
    "  }\n",
    "}\n",
    "\n",
    "n_envs = len(envs)\n",
    "n_agents = len(agents)\n",
    "n_temps = len(temps)\n",
    "\n",
    "n_plots = n_envs * n_agents * n_temps\n",
    "\n",
    "fig, axs = plt.subplots(nrows=len(envs), ncols=len(agents), figsize=(15, 10))\n",
    "\n",
    "if len(envs) * len(agents) > 1:\n",
    "  axs = axs.flatten()\n",
    "\n",
    "for agent_idx, agent in enumerate(agents):\n",
    "  for env_idx, env in enumerate(envs):\n",
    "\n",
    "    env_name = env.unwrapped.spec.id\n",
    "    ax = axs[env_idx * len(agents) + agent_idx] if len(envs) * len(agents) > 1 else axs\n",
    "\n",
    "    for temp in temps:\n",
    "      exp_agent = agent(env,\n",
    "                        temp=temp,\n",
    "                        alpha=alphas[env_name][agent.__name__],\n",
    "                        gamma=gammas[env_name],\n",
    "                        num_tilings=tilings[env_name]['tilings'],\n",
    "                        tiles_per_tiling=tilings[env_name]['tiles'])\n",
    "\n",
    "      results = run_experiment(exp_agent, NUM_RUNS, NUM_EPISODES)\n",
    "\n",
    "      if exp_agent.temp.decay == 1:\n",
    "        label = f'temp={exp_agent.temp}, α={alpha} '\n",
    "      else:\n",
    "        label = f'temp={exp_agent.temp} with decay {exp_agent.temp.decay}^x, α={alpha} '\n",
    "\n",
    "      ax.plot(results, label=label)\n",
    "\n",
    "      std_results = np.std(results, axis=0)\n",
    "      ax.fill_between(range(NUM_EPISODES), results - std_results, results + std_results,\n",
    "                      alpha=0.1)\n",
    "\n",
    "  ax.set_title(\n",
    "    f'{agent.__name__} - {env_name} - {tilings[env_name][\"tilings\"]} tilings - {tilings[env_name][\"tiles\"]} tiles'\n",
    "  )\n",
    "  ax.set_xlabel('Episodes')\n",
    "  ax.set_ylabel('Average Return')\n",
    "\n",
    "  ax.legend()\n",
    "\n",
    "# Add an overall title and adjust layout\n",
    "plt.suptitle('Performance of Expected SARSA and Q-Learning with Different ε and α')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38345b999d27426"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
